{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0645687",
   "metadata": {},
   "source": [
    "# Solving the Taxi-v3 Challenge with Q-Learning üöï\n",
    "\n",
    "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png' alt='Taxi Environment' width='400'>\n",
    "\n",
    "In this project, we'll apply the Q-Learning algorithm to a more challenging environment: `Taxi-v3`. Our goal is to train an agent to efficiently pick up a passenger and transport them to their destination. This notebook will guide you through understanding the environment, implementing the Q-Learning algorithm, tuning hyperparameters, and finally, publishing your trained agent to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86333bd",
   "metadata": {},
   "source": [
    "### üìú For Certification\n",
    "\n",
    "To validate this project for the [Hugging Face Deep RL Course certification](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained Taxi model to the Hub and achieve a result of **>= 4.5**. Your result is calculated as `mean_reward - std_reward` on the official [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ea625",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Installations\n",
    "\n",
    "We begin by installing the required libraries and setting up a virtual display for rendering the environment, which is necessary for creating video replays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy gymnasium pygame imageio tqdm pickle5 huggingface_hub pyvirtualdisplay > /dev/null 2>&1\n",
    "!sudo apt-get update > /dev/null 2>&1\n",
    "!sudo apt-get install -y python3-opengl > /dev/null 2>&1\n",
    "!apt install ffmpeg xvfb > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba347a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac5a49",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Utilities\n",
    "\n",
    "Import the necessary Python libraries and our custom helper functions for evaluation and model publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import helper functions\n",
    "from utils import evaluate_agent, push_to_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeff6c",
   "metadata": {},
   "source": [
    "## Step 3: The Environment - Taxi-v3 üöñ\n",
    "\n",
    "Let's create and explore the `Taxi-v3` environment.\n",
    "\n",
    "üëâ **Documentation**: [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "In this environment, a taxi must navigate a grid to pick up a passenger and drop them off at a designated location. The environment has 500 discrete states, accounting for the taxi's position (25), the passenger's location (5, including inside the taxi), and the destination (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383728ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eae7df",
   "metadata": {},
   "source": [
    "### Understanding the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "print(f\"There are {state_space} possible states and {action_space} possible actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f70cca8",
   "metadata": {},
   "source": [
    "- **Action Space**: `Discrete(6)` corresponds to 6 possible actions:\n",
    "  - 0: `move south`\n",
    "  - 1: `move north`\n",
    "  - 2: `move east`\n",
    "  - 3: `move west`\n",
    "  - 4: `pickup`\n",
    "  - 5: `dropoff`\n",
    "\n",
    "- **Reward System**:\n",
    "  - `-1` for each step.\n",
    "  - `+20` for a successful drop-off.\n",
    "  - `-10` for illegal pickup or drop-off actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e04ae",
   "metadata": {},
   "source": [
    "## Step 4: Building the Q-Learning Algorithm\n",
    "\n",
    "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg' alt='Q-Learning Pseudocode' width='800'/>\n",
    "\n",
    "We'll use the same core Q-Learning functions as before, as the algorithm is general-purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    return np.zeros((state_space, action_space))\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        return np.argmax(Qtable[state][:])\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, learning_rate, gamma):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            Qtable[state][action] += learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de138be",
   "metadata": {},
   "source": [
    "## Step 5: Define Hyperparameters\n",
    "\n",
    "These hyperparameters have been chosen as a good starting point. Feel free to experiment with them to improve your agent's performance!\n",
    "\n",
    "**‚ö†Ô∏è Do not modify `eval_seed`.** This ensures your agent is evaluated on the same starting conditions as everyone else for fair comparison on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 25000\n",
    "learning_rate = 0.7\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Taxi-v3\"\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148]\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fb644",
   "metadata": {},
   "source": [
    "## Step 6: Train the Agent\n",
    "\n",
    "Initialize the Q-table and start the training process. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233be178",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_taxi = initialize_q_table(state_space, action_space)\n",
    "trained_q_table_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, q_table_taxi, learning_rate, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb5611",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate and Publish to Hugging Face Hub üî•\n",
    "\n",
    "Now for the final step: package our trained model and push it to the Hub to get a score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770344f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "    \"qtable\": trained_q_table_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de69098",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"<your-username>\" # FILL THIS\n",
    "repo_name = \"q-Taxi-v3\" # You can choose a different name\n",
    "repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "# The push_to_hub function will evaluate the model, record a video,\n",
    "# and upload everything to the Hub.\n",
    "push_to_hub(repo_id, model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3463d",
   "metadata": {},
   "source": [
    "## Next Steps & Challenges\n",
    "\n",
    "Congratulations on training an agent for `Taxi-v3`! Check the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) to see your score.\n",
    "\n",
    "If your score isn't high enough, here are some ideas to improve it:\n",
    "- **Train for more episodes**: Increase `n_training_episodes`.\n",
    "- **Adjust the learning rate**: A smaller `learning_rate` might lead to more stable learning.\n",
    "- **Tune the exploration decay**: A slower `decay_rate` allows for more exploration, which might be necessary for this larger state space."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
